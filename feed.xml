<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/posts</id>
  <link href="http://blog.url.com/posts"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2017-10-29T12:00:00-04:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Programming with the Modbus RTU &amp; TCP/IP Protocol</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/10/30/programming-with-modbus-rtu-tcp-protocol/"/>
    <id>http://blog.url.com/posts/2017/10/30/programming-with-modbus-rtu-tcp-protocol/</id>
    <published>2017-10-29T12:00:00-04:00</published>
    <updated>2017-11-04T00:14:58-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Today&amp;rsquo;s post probably has a very different audience- modbus protocol; it&amp;rsquo;s nowhere near the web projects that I&amp;rsquo;ve been
doing so far but definitely something I&amp;rsquo;m super interested in. This project mostly works with the &lt;a href="http://www.simplymodbus.ca/FAQ.htm"&gt;modbus protocol&lt;/a&gt;,
which is an open, communication protocol used for transmitting information over serial lines between hardware devices.
Given that IoT is becoming more and more relevant and that the modbus protocol, while old, is still a very commonly used
protocol in the IoT world. So, I hope people will find this post interesting, or even useful if you&amp;rsquo;re attempting something
similar.&lt;/p&gt;

&lt;h3&gt;Backstory&lt;/h3&gt;

&lt;p&gt;The backstory of the project is that I needed a program to read some data from a spindle, as well as control it through an
inverter- the hitachi wj200 over the &lt;a href="http://www.simplymodbus.ca/FAQ.htm"&gt;Modbus&lt;/a&gt; RTU protocol. At the same time, it also needs to relay some of this
information to a &lt;a href="https://www.kepware.com/en-us/products/kepserverex/"&gt;KepwareServer&lt;/a&gt; that acts as both a Modbus TCP/IP slave and a &lt;a href="https://opcfoundation.org/about/opc-technologies/opc-ua/"&gt;OPC/UA&lt;/a&gt; server.
This then, in turn allows communication with other OPC/UA clients.&lt;/p&gt;

&lt;p&gt;The project was initially developed and tested on OSX Sierra 10.12.6 but was eventually compiled and ran on a Windows 10
so that the program can just talk to Kepware over modbus TCP instead of needing 2 machines: 1 linux/OSX + external cabling
to a windows machine (Kepware only runs on windows), but it was also tested on OSX Sierra 10.12.6 first.&lt;/p&gt;

&lt;p&gt;You can find the reference code here: &lt;a href="https://github.com/aranair/modbus_adapter"&gt;https://github.com/aranair/modbus_adapter&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Simplified Demo&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re just here to find some sample code that runs a Modbus client and server, you can check out the &lt;code&gt;simplified&lt;/code&gt; branch
from the repo above. The master and slave code should work with each other.&lt;/p&gt;

&lt;h3&gt;Setup&lt;/h3&gt;

&lt;p&gt;The hardware setup looks roughly like this:&lt;/p&gt;

&lt;p&gt;Spindle &amp;lt;&amp;gt; hitachi wj200 &amp;lt;&amp;gt; USB/COM converter &amp;lt;&amp;gt; C program &amp;lt;&amp;gt; Kepware &amp;lt;&amp;gt; OPC/UA&lt;/p&gt;

&lt;p&gt;In this post though, I&amp;rsquo;ll focus on the first part (from the left) of the setup, up to the C program. The C program
was written and tested on my Mac at first so I&amp;rsquo;ll talk a little bit on that. In the next post, I&amp;rsquo;ll shift the focus to
Kepware and how I compiled the same program in Windows 10 (which turned out to be harder than I thought it should be because of
some dependencies I used).&lt;/p&gt;

&lt;h3&gt;Modbus Masters vs Slaves&lt;/h3&gt;

&lt;p&gt;I am not going to go into details of the Modbus protocol, you can head over &lt;a href="http://www.simplymodbus.ca/FAQ.htm"&gt;here&lt;/a&gt; if you want a quick overview of
the actual protocol like how to &lt;code&gt;write_registers&lt;/code&gt; and &lt;code&gt;write_coil&lt;/code&gt; e.g. but I&amp;rsquo;ll like to talk about something I was
initially confused about.&lt;/p&gt;

&lt;p&gt;It was the concept of masters, slaves, clients and servers in Modbus. The two different ways of
definition that are sometimes used interchangeably in documentations makes it harder to remember which is which, at
least for me. So, before moving ahead with the rest of the stuff, I should probably define it here again so that
it&amp;rsquo;s less confusing for the unfortunate souls who might read on lol.&lt;/p&gt;

&lt;h4&gt;Master / Client&lt;/h4&gt;

&lt;p&gt;The master in a modbus network is the brain that is in charge of controlling devices. They can read and write to
slaves (devices). The concept of master and slave is &lt;a href="https://en.wikipedia.org/wiki/Master/slave_(technology)"&gt;pretty common&lt;/a&gt; in software engineering, so I
won&amp;rsquo;t elaborate more here.&lt;/p&gt;

&lt;p&gt;However, in the case of the modbus protocol, the master is also called the client and physical
devices such as the inverter above, are considered servers, or slaves.  The master would be the
one that initiates the connection to the slaves. I had assumed it was the other way around.&lt;/p&gt;

&lt;p&gt;What remains the same is that, there can only be one master in a single modbus RTU network. (You can
have multiple masters in a modbus TCP/IP network though I think.)&lt;/p&gt;

&lt;h4&gt;Slaves / Server&lt;/h4&gt;

&lt;p&gt;The slaves are the physical devices that you&amp;rsquo;re communicating with. They&amp;rsquo;re also called servers. They
accept connections from the masters.&lt;/p&gt;

&lt;h3&gt;Multiple Modbus Masters?&lt;/h3&gt;

&lt;p&gt;For each of the connections defined in &lt;a href="https://github.com/aranair/modbus_adapter/tree/master/config.cfg"&gt;config.cfg&lt;/a&gt;, I created a Modbus connection.
In this case, one was over RTU protocol and speaks over COM3 and one over TCP/IP.&lt;/p&gt;

&lt;p&gt;My spindle was obviously a slave, and it accepts connections / commands from a master. But, I also needed live
information from the spindle at the windows machine with Kepware. At first, I was hoping that I could achieve
that by having a single Modbus slave to multiple Modbus masters (program + kepware). Unfortunately, that isn&amp;rsquo;t
possible, at least over Modbus RTU.&lt;/p&gt;

&lt;p&gt;To get around that, I got my program to issue commands to the spindle as a master, while periodically polling
whatever required data from it, and relaying that information as a master to another slave- the Kepware instance.&lt;/p&gt;

&lt;p&gt;Essentially, my program initiates and maintains two separate Modbus connections as a master.&lt;/p&gt;

&lt;h3&gt;libconfig&lt;/h3&gt;

&lt;p&gt;With regards to config files setup in my C program, coming Æ’rom ruby and the web environment, YAML seemed like a
natural choice. But I soon learned that, that&amp;rsquo;s not the case in C. I&amp;rsquo;m not sure what is the de-facto solution here,
or if people used config files at all, but I eventually settled on &lt;code&gt;libconfig&lt;/code&gt;. It was fairly simple to use and
the interface was semi-clean I guess, even if a little convoluted.&lt;/p&gt;

&lt;p&gt;It provides you a way to define nested lists and hashes.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;connections = (
  {
    type = "rtu";
    rtu_port = "COM3";
    baud = 115200;
  },
  {
    type = "tcp";
    ip = "127.0.0.1";
    port = 502;
  }
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which you can then get from the program via something like&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;setting = config_lookup(&amp;amp;cfg, "connections");
int connections_count = config_setting_length(setting);
conn_arr = (struct ModbusConn *) malloc(sizeof(struct ModbusConn) * connections_count);

const char *type;
for (i = 0; i &amp;lt; connections_count; i++) {
  config_setting_t *connection = config_setting_get_elem(setting, i);
  config_setting_lookup_string(connection, "type", &amp;amp;type);
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I know, it is a little long if you&amp;rsquo;re coming from ruby since all of those would be a single line of code.
But hey, at least I&amp;rsquo;ve managed to encapsulate all the config stuff into &lt;a href="https://github.com/aranair/modbus_adapter/tree/master/config.h"&gt;config.h&lt;/a&gt;.
From the main program, I just need to search/reference it for the configs!&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;struct ModbusDevice *plc = get_device(config, "hitachiwj200");
struct ModbusDevice *kep = get_device(config, "kepware");
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;libmodbus&lt;/h3&gt;

&lt;p&gt;The library that I was using to establish connections and construct the bytes to send to the devices was &lt;a href="https://github.com/stephane/libmodbus"&gt;libmodbus&lt;/a&gt;,
a library in C.&lt;/p&gt;

&lt;p&gt;The gist of it is, you establish a connection.&lt;/p&gt;
&lt;pre class="highlight c"&gt;&lt;code&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modbus_connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Connection failed: %s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;modbus_strerror&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errno&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="n"&gt;modbus_free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And from there, by addressing directly to the register/coil memory locations you can set or read information through the
protocol.&lt;/p&gt;
&lt;pre class="highlight c"&gt;&lt;code&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;set_coil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modbus_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;uint16_t&lt;/span&gt; &lt;span class="n"&gt;addr_offset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;setting&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Setting coil to %d&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;setting&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modbus_write_bit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;addr_offset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;setting&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;fprintf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Failed to write to coil: %s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;modbus_strerror&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;errno&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The library implements all of the commands the protocol provides. You can read more about the commands at &lt;a href="https://github.com/stephane/libmodbus"&gt;SimplyModbus&lt;/a&gt;.
Each of the commands can be represented via some bytes (as with all things CS lol).&lt;/p&gt;

&lt;p&gt;For instance, the &lt;code&gt;modbus_read_registers&lt;/code&gt; method in &lt;a href="https://github.com/stephane/libmodbus"&gt;libmodbus&lt;/a&gt;, is essentially &lt;code&gt;Read Holding Registers&lt;/code&gt;
on &lt;a href="http://www.simplymodbus.ca/FC03.htm"&gt;this page&lt;/a&gt;. The library helps you take care of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the slave address (which you do have to set beforehand),&lt;/li&gt;
&lt;li&gt;the function code (that represents read_registers) and&lt;/li&gt;
&lt;li&gt;the CRC.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You also have to manually pass in the rest of the parameters such as the memory location and
the number of registers requested.&lt;/p&gt;

&lt;h3&gt;Tricky Memory Addressing&lt;/h3&gt;

&lt;p&gt;And this got a little tricky for me.&lt;/p&gt;

&lt;p&gt;Each type of register / coil also have their designated memory locations and depending on the implementation of the library.
For instance, single register memory locations might start from 40000 or 400001 depending on which library you use, and this
is obviously quite a source of problem.&lt;/p&gt;

&lt;p&gt;Something I found was useful with libmodbus is that it helps you with the first digit of the memory address if you point out
which type it is. You could address a register at memory address 0 with libmodbus and I believe it would automatically map
that to the appropriate memory address, say 400001 in the byte stream for the request it sends out to the slave.&lt;/p&gt;

&lt;p&gt;Do note that different libraries might implement it differently and this can be a source of error in particular.&lt;/p&gt;

&lt;h3&gt;Configuring Kepware&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m (also) not going to go into too much details with the configuration of Kepware since the vast majority of you who
happen to read this article will not be paying the price tag on Kepware. But, I think it&amp;rsquo;s enough to say that,
it is a piece of software that provides multiple drivers and UIs that come bundled with it to allow devices who might
speak different protocols such as modbus, or OPC/UA (and a million others), to speak to each other without
needing another piece of software to translate.&lt;/p&gt;

&lt;p&gt;For the purpose of this project, it was set up on a Windows machine such that it would host a Modbus slave
that accepts connections from my program, and would receive the data over Modbus TCP/IP as a slave and store the
streamed byte data in an internal register that is universally accessible in Kepware by it&amp;rsquo;s other services e.g.
the OPC/UA driver.&lt;/p&gt;

&lt;h3&gt;Virtual Serial Ports Via Pseudo Terminal&lt;/h3&gt;

&lt;p&gt;The above sections kinda ran through the setup that I built. This section is mostly on a quick way to run it locally
without needing a COM port connected to the actual device at first. I found it troublesome to have to test my program
with the actual spindle/hardware connected all the time so I looked for a way to simulate the Modbus RTU locally.&lt;/p&gt;

&lt;p&gt;So far, I&amp;rsquo;ve found that the pseudo terminal works pretty well, okay except when it randomly stops emiting the stream
data mysteriously heh. But, a restart of the socat stuff below usually fixes that.&lt;/p&gt;

&lt;p&gt;I used virtual serial ports to test the program using the steps below:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;$ brew install socat
$ socat -d -d pty,raw,echo=0 pty,raw,echo=0  # to get two pseudo terminals assigned.
$ cat &amp;lt; /dev/ttys035
$ echo "Test" &amp;gt; /dev/ttys037 # on a separate terminal
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="http://www.dest-unreach.org/socat/doc/socat.html"&gt;Socat&lt;/a&gt; is a CLI toolt that allows you to establish two bi-directional byte streams and allows a
transfer of data between them. The commands in the snippet above, in combination, sets up the byte stream across
&lt;code&gt;/dev/ttys035&lt;/code&gt; and &lt;code&gt;/dev/ttys037&lt;/code&gt; (psuedo terminals) so that any data sent from one end of it will be transmitted
over to the other.&lt;/p&gt;

&lt;p&gt;In other words, I could then get my program, which acts as a Modbus RTU master, to connect directly to &lt;code&gt;/dev/ttys035&lt;/code&gt;
that has a Modbus RTU slave connected to it. And they can talk to each other in the modbus protocol flawlessly.&lt;/p&gt;

&lt;h3&gt;Wrapping Up&lt;/h3&gt;

&lt;p&gt;I hope this helps anyone out there who is trying to achieve the same thing and like me, doesn&amp;rsquo;t have a clue how or where
to begin.&lt;/p&gt;

&lt;p&gt;Anyways, after finishing development of the program on my Macbook, I eventually had to move this to a Windows machine running on Win 10.
Despite the fact that C is relatively well-supported on Windows (I mean it&amp;rsquo;s just basically compiling to byte code), I had quite
a hard time compiling it because of all that dll shit and hoops that Windows make you jump through, and some issues surrounding
certain dependencies the program had. I did get everything to compile in MSVS 2017 eventually, but I think I&amp;rsquo;ll leave that story
to Part 2 instead. If you wanna skip ahead, the project files can be found in the &lt;a href="https://github.com/aranair/modbus_adapter/tree/master/win32"&gt;win32 folder&lt;/a&gt;!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Golang Telegram Bot - Migrations, Cronjobs &amp; Refactors</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/08/20/golang-telegram-bot-migrations-cronjobs-and-refactors/"/>
    <id>http://blog.url.com/posts/2017/08/20/golang-telegram-bot-migrations-cronjobs-and-refactors/</id>
    <published>2017-08-19T12:00:00-04:00</published>
    <updated>2017-09-30T20:53:21-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;This post is kind of like a continuation from the previous posts of my Golang Telegram Bot, so if you
haven&amp;rsquo;t seen that yet, it&amp;rsquo;s probably better to start with those first: &lt;a href="https://aranair.github.io/posts/2016/12/25/how-to-set-up-golang-telegram-bot-with-webhooks/"&gt;part 1&lt;/a&gt; and &lt;a href="https://aranair.github.io/posts/2017/01/21/how-i-deployed-golang-bot-on-digital-ocean/"&gt;part 2&lt;/a&gt;. I
basically wanted my telegram bot to be able to remember dated / timed reminders and send messages to
notify me when that time comes (like a calendar). Furthermore, just to force me to complete the tasks
quickly, I also make it repeat the notifications until its cleared.&lt;/p&gt;

&lt;h3&gt;Code Organization&lt;/h3&gt;

&lt;p&gt;Something that I&amp;rsquo;ve never really gotten right in Golang, is code organization.
I find it hard to decide where each piece belongs; it almost feels like a naming- kind of problem to me
and I wish there was a little more convention around this, or a generally accepted framework to think about how
to arrange things.&lt;/p&gt;

&lt;p&gt;When I realised I needed the web-app (for responding to messages/commands) and the timer-app (for
periodically checking the time and sending overdue reminders etc) to run at the same time,
a couple of questions came up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Are these 2 related? (For which the answer is yes - configs, db, handlers)&lt;/li&gt;
&lt;li&gt;Should these two be separate git repos? (No, because of the previous question)&lt;/li&gt;
&lt;li&gt;Can they be run with just one &amp;lsquo;app&amp;rsquo;? (No, reasons in another section)&lt;/li&gt;
&lt;li&gt;They are logically separate &amp;#39;apps&amp;rsquo;, so where should each &lt;code&gt;main.go&lt;/code&gt; be at?&lt;/li&gt;
&lt;li&gt;How do I organise the shared packages and shared configurations?&lt;/li&gt;
&lt;li&gt;How do I structure it such that my Dockerfile and docker-compose configs don&amp;rsquo;t require massive
changes? Or even better, can they be shared? (Yes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While researching, I came across this &lt;a href="https://medium.com/@benbjohnson/structuring-applications-in-go-3b04be4ff091"&gt;blog post&lt;/a&gt; that talks about code organization in
Golang in general and thinking about the application from the perspective of a library, which
all made a ton of sense to me. Head over there to check it out if you&amp;rsquo;re in the same situation as
me.&lt;/p&gt;

&lt;h3&gt;CMD Folder&lt;/h3&gt;

&lt;p&gt;Anyway, so one of the things that was recommended, is to use a &lt;code&gt;cmd&lt;/code&gt; folder to contain
the main running packages (those that actually need a &lt;code&gt;main.go&lt;/code&gt;), thereby removing the main.go
from the root folder. It also satisfies my other criteria of not needing to change my docker
setup drastically, so that&amp;rsquo;s all good.&lt;/p&gt;

&lt;p&gt;Shared packages are left untouched under the root folder so that logically they&amp;rsquo;re like libraries
and exist in some sort of a common area and they can also be easily imported in the timer/webapp packages.&lt;/p&gt;

&lt;p&gt;The general structure comes up to something like this:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;remindbot/
  cmd/
    timer/
      main.go
      ...
    webapp/
      main.go
      ...
  config/
  commands/
  handlers/
  migrations/
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Cron / Scheduled Task&lt;/h3&gt;

&lt;p&gt;I needed a cron that would run perpetually and schedules a task every 5 minutes.&lt;/p&gt;

&lt;p&gt;I feel that this cron job and my webapp should be in somewhat separated. While they are somewhat
related in terms of configs, commands and databases,I felt that they have two rather different
responsibilities.&lt;/p&gt;

&lt;p&gt;I could use a single app, with background tasks or threads running the cron that does exactly
what the timer app does but I&amp;rsquo;ve done them in a way that they run in separate containers,
almost like microservices. I feel that that is a better way of representing the clear distinction
of their responsibilities.&lt;/p&gt;

&lt;p&gt;I use &lt;a href="https://github.com/jasonlvhit/gocron"&gt;gocron&lt;/a&gt; to run a function in a shared package every 5 minutes but if you look at the
code inside, you probably can do without the package if you&amp;rsquo;re afraid of adding dependencies.&lt;/p&gt;

&lt;h3&gt;Migrations&lt;/h3&gt;

&lt;p&gt;I needed to make changes to the database schema; I think there isn&amp;rsquo;t a defacto package for handling
that out there? There are a couple of them out there like goose for example.&lt;/p&gt;

&lt;p&gt;I ended up using &lt;a href="https://github.com/rubenv/sql-migrate"&gt;rubenv/sql-migrate&lt;/a&gt; though; goose was slightly finicky for me, YMMV.
They&amp;rsquo;re also run manually for now since I don&amp;rsquo;t forsee that many migrations to happen but if they start to
become more frequent, I would definitely move them out into a separate docker container that runs
briefly on every deploy.&lt;/p&gt;

&lt;h3&gt;Docker Setup&lt;/h3&gt;

&lt;p&gt;There were minimal changes to my Dockerfile and docker-compose config files.&lt;/p&gt;

&lt;p&gt;For the &lt;code&gt;docker-compose.yml&lt;/code&gt;, I&amp;rsquo;ve added a &lt;code&gt;base&lt;/code&gt; key that builds the Dockerfile in the root
folder. And then each of the other 2 services would just define a different entrypoint. I could
also have two separate Dockerfiles but at this point I think they&amp;rsquo;re still similar enough to just
have one Dockerfile.&lt;/p&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="na"&gt;version&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="s"&gt;2'&lt;/span&gt;
&lt;span class="na"&gt;services&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
  &lt;span class="na"&gt;base&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;build&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
  &lt;span class="na"&gt;hazel&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;extends&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;base&lt;/span&gt;
    &lt;span class="na"&gt;ports&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="s"&gt;8080:8080"&lt;/span&gt;
    &lt;span class="na"&gt;expose&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="s"&gt;8080"&lt;/span&gt;
    &lt;span class="na"&gt;volumes&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s"&gt;/var/data:/var/data&lt;/span&gt;
    &lt;span class="na"&gt;entrypoint&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s"&gt;webapp&lt;/span&gt;
  &lt;span class="na"&gt;timer&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;extends&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;base&lt;/span&gt;
    &lt;span class="na"&gt;volumes&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s"&gt;/var/data:/var/data&lt;/span&gt;
    &lt;span class="na"&gt;entrypoint&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s"&gt;timer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;ve also setup &lt;a href="https://github.com/tools/godep"&gt;Godep&lt;/a&gt; to deal with external package version control. It does a simple job -
save the external packages into the vendor folder so that they can be restored easily the next time.&lt;/p&gt;

&lt;p&gt;That way, the Dockerfile would have just one package to grab and restore all the package locally,
instead of getting all of them via &lt;code&gt;go get&lt;/code&gt;. Other than that, the Dockerfile basically remains
unchanged other than the Godep stuff and moving the entrypoint from before into the docker-compose
instead.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;FROM golang:1.6

ADD configs.toml /go/bin/
ADD . /go/src/github.com/aranair/remindbot
WORKDIR /go/src/github.com/aranair/remindbot

RUN go get github.com/tools/godep
RUN godep restore
RUN go install ./...

WORKDIR /go/src/github.com/aranair/remindbot
WORKDIR /go/bin/
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Next Iterations&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I want to be able to use &amp;ldquo;today&amp;rdquo; / &amp;ldquo;tomorrow&amp;rdquo; / &amp;ldquo;next week&amp;rdquo; instead of having to put in a date
manually; this probably just means better datetime parsing.&lt;/li&gt;
&lt;li&gt;Ideally, I also want a snooze function, where you can postpone the notifications by X number of
hours.&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>Building a Python CLI Stock Ticker with Urwid</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/06/28/building-a-python-cli-stock-ticker-with-urwid/"/>
    <id>http://blog.url.com/posts/2017/06/28/building-a-python-cli-stock-ticker-with-urwid/</id>
    <published>2017-06-27T12:00:00-04:00</published>
    <updated>2017-09-30T20:53:19-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;A bit of context - I do some investing in equities on the side and I&amp;rsquo;ve always wanted to build a simple stock ticker in the form of a
CLI app that runs in my terminal setup. There were a few out there but none that would show 
just the information I needed, in a minimalistic fashion. And I thought it would be a fun project 
for me since I don&amp;rsquo;t have much prior experience building a CLI app. &lt;/p&gt;

&lt;p&gt;So last weekend, I decided to build one for fun! Here is the quick image of it running and 
you can find the code over at &lt;a href="https://github.com/aranair/rtscli"&gt;https://github.com/aranair/rtscli&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aranair/rtscli/master/rtscli-demo.png"&gt;&lt;img src="https://raw.githubusercontent.com/aranair/rtscli/master/rtscli-demo.png" alt="Demo" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Python &amp;amp; CLI Libraries&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;ve been starting to work with Python recently - due to the data-related work at Pocketmath.
So language-wise, Python was a natural choice. But honestly, many other languages offer packages that 
can achieve the same result or more - like the &lt;a href="http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/intro.html#WHATIS"&gt;ccurses&lt;/a&gt; library in C.&lt;/p&gt;

&lt;p&gt;But for Python, I found a number of different libraries for CLIs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/2/howto/curses.html"&gt;curses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://urwid.org/"&gt;urwid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://click.pocoo.org/5/"&gt;click&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/thomasballinger/curtsies"&gt;curtsies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Urwid&lt;/h3&gt;

&lt;p&gt;Eventually I went with &lt;a href="http://urwid.org/"&gt;urwid&lt;/a&gt; because it seems easier to just jump in and get started with it instantly.
Urwid is an alternative to the &lt;a href="https://docs.python.org/2/howto/curses.html"&gt;curses&lt;/a&gt; library in Python and it implements sort of like a layer
ontop of boilerplate stuff that turns out to be really productive for me.&lt;/p&gt;

&lt;h3&gt;Stock Ticker - Details&lt;/h3&gt;

&lt;p&gt;Okay, this section is mainly describing some of the functionalities I wanted and strictly
speaking, has nothing to do with urwid nor python nor code so feel free to skip if you&amp;rsquo;re not into this ;)&lt;/p&gt;

&lt;p&gt;The basic functionalities I wanted was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Read a list of stock tickers that contain the following:

&lt;ul&gt;
&lt;li&gt;Name of the Stock&lt;/li&gt;
&lt;li&gt;Symbol (google symbol - &lt;code&gt;HKG:0005&lt;/code&gt; e.g.)&lt;/li&gt;
&lt;li&gt;Buy-in price&lt;/li&gt;
&lt;li&gt;Number of shares held&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Display key information per stock:

&lt;ul&gt;
&lt;li&gt;Change (day)&lt;/li&gt;
&lt;li&gt;% Change (day)&lt;/li&gt;
&lt;li&gt;Gain (overall)&lt;/li&gt;
&lt;li&gt;% Gain (overall)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Display a portfolio wide change&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Implementation - MainLoop&lt;/h3&gt;

&lt;p&gt;I imagined the app to be a long-running CLI that continuously accepts commands, while at the same time
pulling the stock information at an interval, on top of re-painting the information on screen. That 
can be modelled with a loop - a &lt;code&gt;MainLoop&lt;/code&gt; as urwid calls it.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;urwid&lt;/span&gt;
&lt;span class="n"&gt;main_loop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MainLoop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layout&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;palette&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unhandled_input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;handle_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;main_loop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_alarm_in&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;refresh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;main_loop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above instantiates a &lt;code&gt;MainLoop&lt;/code&gt; which ties together a display module, some widgets
and an event loop. Quoting the documentation: &lt;em&gt;It handles passing input from the display module to the 
widgets, rendering the widgets and passing the rendered canvas to the display module to be drawn.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I think of it as a controller of sorts.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Implementation - Refresh Mechanism&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;set_alarm_in&lt;/code&gt; is like &lt;code&gt;setTimeout&lt;/code&gt; in the JavaScript world; it just calls the &lt;code&gt;refresh&lt;/code&gt; method instantly
in this case. In the refresh method I set another alarm that goes off in &lt;code&gt;10s&lt;/code&gt;, that is as good as 
telling it to do one data pull from Google Finance every 10 seconds.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;refresh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_loop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;main_loop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw_screen&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;quote_box&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;base_widget&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_update&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;main_loop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_alarm_in&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;refresh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It calls the &lt;code&gt;get_update()&lt;/code&gt; method that spits out an array of tuples of &lt;code&gt;(color_scheme, text)&lt;/code&gt;. I&amp;rsquo;ll
skip the details of the method but it is basically just calling a REST api that replies with JSON and
parsing the response into a long string for display.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This method kind of interrupts the event loop every 10 seconds.&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Color Palette&lt;/h3&gt;

&lt;p&gt;I also define a color scheme that can be used throughout the app.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c"&gt;# Tuples of (Key, font color, background color)&lt;/span&gt;
&lt;span class="n"&gt;palette&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'titlebar'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark red'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'refresh button'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark green,bold'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'quit button'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark red'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'getting quote'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark blue'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'headers'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'white,bold'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'change '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark green'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'change negative'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'dark red'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other parts of the app where text is displayed, those keys can be used to tell the app
what color / background the text span should be rendered in.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c"&gt;# Notice "refresh button" and "quit button" keys were defined above in the color scheme.&lt;/span&gt;
&lt;span class="n"&gt;menu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="s"&gt;u'Press ('&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'refresh button'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u'R'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u') to manually refresh. '&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;u'Press ('&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'quit button'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;u'Q'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;u') to quit.'&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;This is just like a color palette of a site&lt;/strong&gt;&lt;/p&gt;

&lt;h3&gt;Layout&lt;/h3&gt;

&lt;p&gt;This creates a header and assigns the &lt;code&gt;titlebar&lt;/code&gt; key color scheme to it.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;header_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;u' Stock Quotes'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AttrMap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;header_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;'titlebar'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same for &lt;code&gt;quote_text&lt;/code&gt;, except this time a bunch of other widgets were used.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="n"&gt;quote_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;u'Press (R) to get your first quote!'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;quote_filler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Filler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;quote_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valign&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'top'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;v_padding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Padding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;quote_filler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;quote_box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LineBox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v_padding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;a href="http://urwid.org/reference/widget.html?highlight=filler#filler"&gt;Filler&lt;/a&gt; widget will maximise itself to the screen and the &lt;a href="http://urwid.org/reference/widget.html?highlight=padding#padding"&gt;Padding&lt;/a&gt; one is self-explanatory;
it sets padding in terms of columns. And finally the &lt;code&gt;LineBox&lt;/code&gt; leaves a border around the components nested in it.
Finally, the &lt;a href="http://urwid.org/reference/widget.html?highlight=frame#frame"&gt;Frame&lt;/a&gt; ties it all up into a layout for my app and it is used in the initialization
of the &lt;code&gt;MainLoop&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c"&gt;# Assemble the widgets&lt;/span&gt;
&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;quote_box&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;footer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;menu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# main_loop = urwid.MainLoop(layout, palette, unhandled_input=handle_input)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a ton of other ways you can structure the main components of the app and there are so many widgets
implemented in the library like the &lt;a href="http://urwid.org/manual/widgets.html#container-widgets"&gt;container widgets&lt;/a&gt; - where you have 
Piles(stacking vertically) or ListBox (for menus) for example.&lt;/p&gt;

&lt;h3&gt;Implementation - User Interaction&lt;/h3&gt;

&lt;p&gt;I didn&amp;rsquo;t really need much user interaction so that made things alot easier; the only interaction 
was basically just 2 keys: &lt;code&gt;R&lt;/code&gt; - which forces a refresh of the data and &lt;code&gt;Q&lt;/code&gt; - which exits the program.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="c"&gt;# Handle key presses&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;handle_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'R'&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'r'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;refresh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;main_loop&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;''&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'Q'&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;'q'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;urwid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExitMainLoop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The method has to accept the 2 keys. That&amp;rsquo;s the contract of the &lt;code&gt;main_loop.set_alarm_in()&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Preparing the Package for Pypi&lt;/h3&gt;

&lt;p&gt;I added a &lt;code&gt;setup.py&lt;/code&gt;, this is sort of like the &lt;code&gt;gemspec&lt;/code&gt; of ruby gems where metadata and dependencies
are stated. I didn&amp;rsquo;t use any folders because the script is so short already.&lt;/p&gt;
&lt;pre class="highlight python"&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;setuptools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;setup&lt;/span&gt;
&lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'rtscli'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'0.3'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'A realtime stocks ticker that runs in CLI'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'http://github.com/aranair/rtscli'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;author&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'Boa Ho Man'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;author_email&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'boa.homan@gmail.com'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;license&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;'MIT'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;install_requires&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s"&gt;'urwid'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'HTMLParser'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;'simplejson'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;zip_safe&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;py_modules&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'rtscli'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;entry_points&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s"&gt;'console_scripts'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;'rtscli=rtscli:cli'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, you do have to setup a &lt;a href="https://pypi.python.org/pypi"&gt;pypi&lt;/a&gt; account and have the credentials in the &lt;code&gt;.pypirc&lt;/code&gt; file.
After that, all that was left was:&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="gp"&gt;$ &lt;/span&gt;python setup.py register -r pypi
&lt;span class="gp"&gt;$ &lt;/span&gt;python setup.py sdist upload -r pypi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&amp;rsquo;s it; All ready for &lt;code&gt;pip install rtscli&lt;/code&gt;!&lt;/p&gt;

&lt;h3&gt;Next Iteration&lt;/h3&gt;

&lt;p&gt;There are a bunch of improvements I can think of, but for a start:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Perhaps, most importantly, I would like to refactor this into using proper widgets 
instead of appending texts into the quote_box.&lt;/li&gt;
&lt;li&gt;Grab news from Google related to the stock tickers and displays them in a separate window OR&lt;/li&gt;
&lt;li&gt;Track transactions (but this is a lot more complicated that I have time for :P)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have any suggestions, feel free to let me know below!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Upgrading to ElasticSearch 5.2.2 on Amazon ECS</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/04/03/getting-elasticsearch-5.2.2-to-work-on-amazon-ecs/"/>
    <id>http://blog.url.com/posts/2017/04/03/getting-elasticsearch-5.2.2-to-work-on-amazon-ecs/</id>
    <published>2017-04-02T12:00:00-04:00</published>
    <updated>2017-09-30T20:53:19-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;In &lt;a href="https://github.com/docker-library/elasticsearch/issues/111"&gt;one of my previous post&lt;/a&gt;, I talked about how I set up Elasticsearch 2.3.5 on ECS. I got a comment in
that post that prompted me to update the setup for Elasticsearch 5. It&amp;rsquo;s been awhile, but better late than never right?
So I gave it a go! In this post I&amp;rsquo;ll like to share what I found in the process.&lt;/p&gt;

&lt;p&gt;There were a couple of other configuration changes that were required to upgrade to 5.2.2 from 2.3.5 but they weren&amp;rsquo;t
difficult, except one that may potentially deter you from using ECS with Elasticsearch 5, for the time being at least.&lt;/p&gt;

&lt;h3&gt;Main Caveat&lt;/h3&gt;

&lt;p&gt;At this point, I&amp;rsquo;ll mention a caveat that will likely save you an hour of headache and trouble.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Long story short, You will need to SSH into the ECS instances to run the command on the parent to 
get past the error message below. I am not aware of any other solutions but if you do, feel free to
let me know in the comments section below!&lt;/em&gt;&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;elasticsearch:5.2.2 max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;a href="https://github.com/docker-library/elasticsearch/issues/111"&gt;docker-library/elasticsearch github issue&lt;/a&gt; suggests running &lt;code&gt;sudo sysctl -w vm.max_map_count=262144&lt;/code&gt; or run the docker 
container with the &lt;code&gt;--sysctl&lt;/code&gt; option to fix this problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;, because of how ECS implements the agents currently, many of the docker run options are not available. 
This is well-documented over at the &lt;a href="https://github.com/aws/amazon-ecs-agent/issues/502"&gt;amazon-ecs-agent Github repository&lt;/a&gt; so I won&amp;rsquo;t echo them here. But it does 
seem like there are a bunch of others who are encountering the same issue.&lt;/p&gt;

&lt;h3&gt;Continuing..&lt;/h3&gt;

&lt;p&gt;In my opinion, this makes the combination slightly less-than-ideal because the manual configuration work that is required 
on the EC2 instances takes away some of the benefits of implementing ElasticSearch in ECS.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re okay with the manual configuration running that command on the instances, or for example, if you plan to provision
a few instances and leave them there for awhile, then this hiccup would deal no damage. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Let&amp;rsquo;s forge on.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Configuration Changes&lt;/h3&gt;

&lt;p&gt;The starting point is the &lt;a href="https://github.com/aranair/docker-elasticsearch-ecs/blob/master/docker-elasticsearch/Dockerfile"&gt;Dockerfile&lt;/a&gt; for ElasticSearch 2.3.5 in my &lt;a href="https://github.com/aranair/docker-elasticsearch-ecs"&gt;docker-elasticsearch-ecs repo&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;FROM elasticsearch:2.3

WORKDIR /usr/share/elasticsearch

RUN bin/plugin install cloud-aws
RUN bin/plugin install mobz/elasticsearch-head
RUN bin/plugin install analysis-phonetic

COPY elasticsearch.yml config/elasticsearch.yml
COPY logging.yml config/logging.yml
COPY elasticsearch-entrypoint.sh /docker-entrypoint.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;And modified to:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;FROM elasticsearch:5.2.2

COPY elasticsearch.yml config/elasticsearch.yml
COPY logging.yml config/logging.yml
COPY elasticsearch-entrypoint.sh /docker-entrypoint.sh

RUN bin/elasticsearch-plugin install discovery-ec2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notable changes include bumping the version and changing &lt;code&gt;cloud-aws&lt;/code&gt; plugin to &lt;code&gt;discovery-ec2&lt;/code&gt; which
is the new plugin for the same purpose of node discovery in cloud environments.&lt;/p&gt;

&lt;h3&gt;File Descriptors and Ulimits&lt;/h3&gt;

&lt;p&gt;I needed to change the docker-compose file slightly to include the &lt;code&gt;ulimits&lt;/code&gt;. It is a new mandatory configuration item.
You can find out more in &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setting-system-settings.html"&gt;this documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;version: '2'
services:
  data:
    build: ./docker-data/
    volumes:
      - /usr/share/elasticsearch/data

  search:
    build: ./docker-elasticsearch/
    volumes_from:
      - data
    ports:
      - "9200:9200"
      - "9300:9300"
    ulimits:
        nofile:
           soft: 65536
           hard: 65536
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;elasticsearch.yml&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;plugin.mandatory: cloud-aws&lt;/code&gt; and &lt;code&gt;discovery.type: EC2&lt;/code&gt; and &lt;code&gt;discovery.zen.ping.multicast.enabled: false&lt;/code&gt; has been
removed or modified to the following below.&lt;/p&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="s"&gt;script.inline&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;true&lt;/span&gt;
&lt;span class="s"&gt;bootstrap.memory_lock&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;false&lt;/span&gt;
&lt;span class="s"&gt;network.host&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;0.0.0.0&lt;/span&gt;
&lt;span class="s"&gt;network.publish_host&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;_ec2:privateIp_&lt;/span&gt;
&lt;span class="s"&gt;discovery.zen.hosts_provider&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;ec2&lt;/span&gt;
&lt;span class="s"&gt;discovery.ec2.groups&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;dockerecs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Task Definition / Heap Size&lt;/h3&gt;

&lt;p&gt;In Elasticsearch 5, the heap size is also a mandatory configuration. For this, I set it directly in ECS via 
the JSON task definition. I had to set the &lt;code&gt;ES_JAVA_OPTS&lt;/code&gt; for it to work.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nv"&gt;ES_JAVA_OPTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"-Xms1g -Xmx1g"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Wrapping up&lt;/h3&gt;

&lt;p&gt;It isn&amp;rsquo;t a whole lot of changes but it did take some time googling each of the issues that came up as I tried to start
the services on ECS and also eventually had to SSH into the instance to set the &lt;code&gt;vm.max_map_count&lt;/code&gt; before I managed to
get the cluster up.&lt;/p&gt;

&lt;p&gt;This is obviously less than ideal in a deployment process which otherwise could be full-automated. But if you&amp;rsquo;re 
still looking ahead to use ElasticSearch 5 in ECS, I hope the above steps serve you well!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Tuning My Apache Spark Data Processing Cluster on Amazon EMR</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/03/10/tuning-my-apache-spark-cluster-on-aws-emr/"/>
    <id>http://blog.url.com/posts/2017/03/10/tuning-my-apache-spark-cluster-on-aws-emr/</id>
    <published>2017-03-09T11:00:00-05:00</published>
    <updated>2017-09-30T20:53:19-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Lately, I had the chance to work on some data integration at Pocketmath where I wrote a 
bunch of Spark scripts in Scala to run some transformations on a data set of about 
250GB that will run on a monthly basis. In this post, I talk about some of the 
problems I encountered, and some considerations while setting up the cluster and also how I improved
the performance of the the Spark tasks.&lt;/p&gt;

&lt;h3&gt;Dataset Size&lt;/h3&gt;

&lt;p&gt;The size of the data set is only 250GB, which probably isn&amp;rsquo;t even close to the scale other data 
engineers handle, but is easily one of the bigger sets for me. Nonetheless, I do think the 
transformations are on the heavy side; it involves a chain of rather expensive operations. &lt;/p&gt;

&lt;h3&gt;Multiple Shuffle Operations&lt;/h3&gt;

&lt;p&gt;The sequence of execution is something like &lt;code&gt;JOIN&lt;/code&gt;, &lt;code&gt;JOIN&lt;/code&gt;, &lt;code&gt;UNION&lt;/code&gt;, &lt;code&gt;EXPLODE&lt;/code&gt;, &lt;code&gt;SORT&lt;/code&gt; within partition,
then &lt;code&gt;GROUP&lt;/code&gt;, &lt;code&gt;COLLECT&lt;/code&gt; and finally another &lt;code&gt;SORT&lt;/code&gt; eventually. &lt;/p&gt;

&lt;p&gt;Each of these are &lt;em&gt;pretty&lt;/em&gt; expensive shuffle operations. Not surprisingly, these operations posed some problems
even at this moderate data size.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://s3-ap-southeast-1.amazonaws.com/homan/blog/spark-flow.png"&gt;&lt;img src="https://s3-ap-southeast-1.amazonaws.com/homan/blog/spark-flow-small.png" alt="Spark Flow Chart" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Another Difficulty&lt;/h3&gt;

&lt;p&gt;On top of that, I also had to split a file that looks like the examples below. &lt;/p&gt;

&lt;p&gt;Essentially, I think of it as generating ranks or row numbers (of course, with a bunch of other data transformations)&lt;/p&gt;

&lt;p&gt;From this format:&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;51abebcfab2ef2abeed2f 8,120,384,898 
21abfbbeef5791adef3f2 1,9,1214,8827 
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Into these two files:&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="c"&gt;# 1,2 are like IDs&lt;/span&gt;
51abebcfab2ef2abeed2f 1
21abfbbeef5791adef3f2 2
...
&lt;/code&gt;&lt;/pre&gt;&lt;pre class="highlight shell"&gt;&lt;code&gt;1 8,120,384,898 
2 1,9,1214,8827 

...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that generating a consistent row number like this is a difficult operation for Spark to handle.
&lt;strong&gt;Matter of fact&lt;/strong&gt;, it is probably an expensive task for any distributed system to perform. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Dataframes&lt;/code&gt; are available in Spark 2.0 and I mainly use that data structure. The only way that I know of currently to 
generate these row numbers with a Dataframe is to first convert into an RDD and do a &lt;code&gt;zipWithIndex&lt;/code&gt; on it. &lt;/p&gt;
&lt;pre class="highlight scala"&gt;&lt;code&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;segRdd&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;segmentIdGroups&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rdd&lt;/span&gt;
&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;segRdd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zipWithIndex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Row&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Long&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nc"&gt;Row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromSeq&lt;/span&gt;&lt;span class="o"&gt;((&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+:&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toSeq&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Okay, there is actually another method that involves windows and partitions but unfortunately it basically 
moves all the data into one partition, which isn&amp;rsquo;t feasible for me.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Executer Cores and Memory Allocation&lt;/h3&gt;

&lt;p&gt;While starting the Spark task in Amazon EMR, I manually set the &lt;code&gt;--executor-cores&lt;/code&gt; and &lt;code&gt;--executor-memory&lt;/code&gt; 
configurations. The calculation is somewhat non-intuitive at first because I have to manually take into account 
the overheads of YARN, the application master/driver cores and memory usage et cetera.&lt;/p&gt;

&lt;p&gt;As a guideline, YARN overheads take roughly &lt;code&gt;7%&lt;/code&gt; and allocating more from there is generally good practice. This
ensures enough is left for system processes and Hadoop e.g.&lt;/p&gt;

&lt;p&gt;Using an instance of &lt;code&gt;r3.8xlarge&lt;/code&gt; as an example:&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="c"&gt;# 122GB RAM, 16 cores&lt;/span&gt;

&lt;span class="nv"&gt;$CORES_PER_DRIVER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;4
&lt;span class="nv"&gt;$MEMORY_PER_DRIVER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;24G

&lt;span class="nv"&gt;$CORES_PER_EXECUTOR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;4
&lt;span class="nv"&gt;$MEMORY_PER_EXECUTOR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;24G
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each instance could potentially run 4 executors, with 4 cores per executor. Available memory for the HEAP is 122/4 = 30.5G.&lt;/p&gt;

&lt;p&gt;To account for the overheads, I multiply the available memory by &lt;strong&gt;0.93&lt;/strong&gt;. This works out to be &lt;strong&gt;28G&lt;/strong&gt;.
For my experiment, I used &lt;strong&gt;24G&lt;/strong&gt; just to account for the overheads but on hindsight, &lt;strong&gt;26G&lt;/strong&gt; should be enough too.&lt;/p&gt;

&lt;p&gt;Recap: &lt;/p&gt;
&lt;pre class="highlight scala"&gt;&lt;code&gt;&lt;span class="n"&gt;total_memory_available&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total_cores_available&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;// 122GB/16 * (1 - 0.07) = 28G
// Leave some leeway, to 24-26G
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;This directly affects how many executors that can be deployed per instance and also affects the
memory available for each task, and consequently for each shuffle operation.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Task / Partition Size&lt;/h3&gt;

&lt;p&gt;Another critical configuration is the task size; it is something that I think should be considered carefully 
because the task &lt;strong&gt;will slow down&lt;/strong&gt; by quite a bit if it starts to spill to disk.&lt;/p&gt;

&lt;p&gt;Initially, I just set the &lt;code&gt;default_parallelism&lt;/code&gt; in Spark and expected the system to automatically 
handle the rest, and was surprised to see some stages spilling to disk causing the cluster to slow down. 
I later found out that &lt;code&gt;default_parallelism&lt;/code&gt; is only used for certain operations and for the rest of the time, 
Spark would infer the size by looking at the input from a previous stage, which happens to be the number of files
it reads: 300. &lt;/p&gt;

&lt;p&gt;Each of these files are roughly 800-900MB. And having that few paritions is not ideal since 
each of these tasks / partitions will be too big to fit into memory. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Well alright, this actually depends on your executor setup too.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I had to force a repartition via &lt;code&gt;df.repartition(2000)&lt;/code&gt; right after the reading of the files. 
This would immediately add a shuffle step but performs better later on in other tasks in my opinion, YMMV though.&lt;/p&gt;

&lt;h3&gt;Shuffle Memory Usage, Executor Memory-to-CPU ratio&lt;/h3&gt;

&lt;p&gt;In general, I tried to optimize the system to avoid any form of spilling, both memory and disk. If
the entire shuffle operation can fit into memory, there will be no spilling.&lt;/p&gt;

&lt;p&gt;Each core in an executor runs a single task at any one time. Hence, with 26GB per executor and 4 cores each executor, 
the HEAP_SIZE allocated for each task is &lt;strong&gt;26G/4&lt;/strong&gt; or &lt;strong&gt;4G&lt;/strong&gt;.  &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;However, not all the memory allocated to the executor is used for shuffle operations.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The memory available for shuffle can be calculated as such:&lt;/p&gt;
&lt;pre class="highlight scala"&gt;&lt;code&gt;&lt;span class="c1"&gt;// Per task
&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.96&lt;/span&gt;&lt;span class="nc"&gt;GB&lt;/span&gt;

&lt;span class="c1"&gt;// 0.2 -&amp;gt; spark.shuffle.memoryFraction
// 0.8 -&amp;gt; spark.shuffle.safetyFraction
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your task is already spilling to disk, try using this formula to find out how much space it
actually needs. This might help you to better fine tune the RAM-to-CPU ratio for ur executor tasks.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;shuffle_write &lt;span class="k"&gt;*&lt;/span&gt; shuffle_spill_mem &lt;span class="k"&gt;*&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;4&lt;span class="o"&gt;)&lt;/span&gt;executor_cores
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
shuffle_spill_disk &lt;span class="k"&gt;*&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;24&lt;span class="o"&gt;)&lt;/span&gt;executor_mem &lt;span class="k"&gt;*&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0.2&lt;span class="o"&gt;)&lt;/span&gt;shuffle_mem_fraction &lt;span class="k"&gt;*&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;0.8&lt;span class="o"&gt;)&lt;/span&gt;shuffle_safety_fraction
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Splitting the task size properly is probably one of the bigger improvements while tuning my cluster. 
&lt;strong&gt;Key takeaway:&lt;/strong&gt; It is definitely better to err on a higher number of partitions, which results in a smaller task bite size.&lt;/p&gt;

&lt;h3&gt;Counter Example&lt;/h3&gt;

&lt;p&gt;To quickly illustrate how things can go wrong in a problematic configuration, 
I&amp;rsquo;ll use one of my iterations as an example.&lt;/p&gt;

&lt;p&gt;I first used &lt;code&gt;c3&lt;/code&gt; &amp;amp; &lt;code&gt;m3&lt;/code&gt; instances and only allocated &lt;code&gt;10G&lt;/code&gt; per 3 cores. This works out
to be about 500MB for shuffling each task. I had only 300 partitions and my task sizes were well beyond 850MB. &lt;/p&gt;

&lt;p&gt;It resulted in a ton more needless computations:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://s3-ap-southeast-1.amazonaws.com/homan/blog/spark-shuffle-spill.png"&gt;&lt;img src="https://s3-ap-southeast-1.amazonaws.com/homan/blog/spark-shuffle-spill-small.png" alt="Shuffle Spills" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3&gt;Getting the Right Partition Size and Instance Type&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;It is crucial to get the partition size right for it to run smoothly, but getting the right instance type
makes it much more efficient.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Back to my setup, with about 2000 paritions and 250GB data, each partition or task works out to be 
only about 125MB, which is close to the 128MB that is recommended in the official docs. &lt;/p&gt;

&lt;p&gt;At that partition size, it is more efficient to run &lt;code&gt;c3.8xlarge&lt;/code&gt; instances with 
a lower memory to core ratio. I did choose to use &lt;code&gt;i2.2xlarge&lt;/code&gt; memory instances to
eliminate any possibility of a memory constraint issue but the &lt;code&gt;c3.8xlarge&lt;/code&gt; would&amp;rsquo;ve been much faster.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="c"&gt;# c3.8xlarge: 32 VCPUS, 60GB Memory&lt;/span&gt;
&lt;span class="nv"&gt;$CORES_PER_EXECUTOR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;4
&lt;span class="nv"&gt;$MEMORY_PER_EXECUTOR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;6.5G

&lt;span class="c"&gt;# Available memory for shuffle, more than enough for 125mb&lt;/span&gt;
6.5 / 4 &lt;span class="k"&gt;*&lt;/span&gt; 0.2 &lt;span class="k"&gt;*&lt;/span&gt; 0.8 &lt;span class="o"&gt;=&lt;/span&gt; 0.26G
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Spot Instances and HDFS&lt;/h3&gt;

&lt;p&gt;Amazon EMR allows you to bid for spot instances at a &lt;em&gt;fraction of the cost&lt;/em&gt; of the original instance
price. I use them frequently and have found them to be massively discounted during some hours. &lt;/p&gt;

&lt;p&gt;I had HDFS running for the cluster and the results of each &lt;code&gt;result stage&lt;/code&gt; are stored into the HDFS for future use. 
At first, I ran a test using spot instances completely, even for the CORE instance group, which turned out to be
a big mistake.&lt;/p&gt;

&lt;p&gt;When I lose the instances, inevitably, from getting outbidded during peak hours, the cluster loses data. 
In my experience, Spark is unable to fully recover from the lost data, even after taking extra time for 
stage-retries.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;With everything taken into consideration, I found it easier to just use on-demand instances 
for the CORE instance group entirely.&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Instance Setup&lt;/h3&gt;

&lt;p&gt;I recommend using non-spot, disk-optimized instances for the CORE instance group. For example, I got two 800GB SSDs 
with an &lt;code&gt;i2.2xlarge&lt;/code&gt; which costs only ~$1.70 per hour. In comparison, &lt;code&gt;c3&lt;/code&gt; or &lt;code&gt;r3&lt;/code&gt; instances 
give you way lesser disk space.&lt;/p&gt;

&lt;p&gt;For the actual computation in the task instance group, I would switch to using only spot instances 
(&lt;code&gt;r3.8xlarge&lt;/code&gt; or &lt;code&gt;c3.8xlarge&lt;/code&gt;). I&amp;rsquo;ve found that this was the most cost-efficient to run my task.&lt;/p&gt;

&lt;p&gt;To bring it all together, I used 20 spot instances of &lt;code&gt;r3.8xlarge&lt;/code&gt;. Mentioned this above, but I&amp;rsquo;ll say it again.
Memory instances were used just to eliminate any potential issue of shuffle spills but &lt;code&gt;c3.8xlarge&lt;/code&gt; would&amp;rsquo;ve been more efficient
definitely as I&amp;rsquo;ve showned above. For the core group, there were 3 &lt;code&gt;i2.2xlarge&lt;/code&gt; on-demand instances for the HDFS.&lt;/p&gt;

&lt;h3&gt;To Persist or Not?&lt;/h3&gt;

&lt;p&gt;In some of the heavy shuffles, I found that it was faster to persist them on disk to 
prevent re-calculations. This is especially true if you&amp;rsquo;re re-using scala variables further
down the chain. Obviously, you&amp;rsquo;ll need to look into the total calculation time and compare it with
the network read bytes (divided by an average network throughput) to see if it is worth while to
persist. &lt;/p&gt;

&lt;p&gt;One good thing is that the Amazon EMR handles the HDFS integrations seamlessly which makes
it effortless to do a &lt;code&gt;DISK_ONLY&lt;/code&gt; persistance. One thing to note is that since I was doing some disk persistance, 
I do end up using more disk space than the total data size. This is also why I chose to use the &lt;code&gt;i2&lt;/code&gt; instances
for HDFS.&lt;/p&gt;

&lt;h3&gt;Recalculations vs Persistence&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Recalculations will yield different sequences every time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In my case, having to generate a key for each row was the one requirement I couldn&amp;rsquo;t remove. &lt;/p&gt;

&lt;p&gt;Without persisting it to disk first, using the variable again would cause a re-calculation 
through the stages and &lt;code&gt;zipWithIndex&lt;/code&gt; could potentially produce results which are 
different each time and be rendered absolutely useless. &lt;/p&gt;

&lt;p&gt;I was forced to persist them to disk but either way, since they were heavy operations, persisting
them to disk made sense too.&lt;/p&gt;

&lt;h3&gt;Resources&lt;/h3&gt;

&lt;p&gt;All of the information has been sourced from multiple sources including, but not limited to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/"&gt;Cloudera, How to tune your Apache Spark Jobs Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/"&gt;Cloudera, How to tune your Apache Spark Jobs Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details"&gt;Mastering Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/pulse/tune-spark-jobs-2-chaaranpall-lambba"&gt;Tune Spark Jobs 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/cloudera/top-5-mistakes-to-avoid-when-writing-apache-spark-applications"&gt;Top 5 Mistakes while writing Apache Spark Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  <entry>
    <title>How I Deployed My Golang Telegram Bot</title>
    <link rel="alternate" href="http://blog.url.com/posts/2017/01/21/how-i-deployed-golang-bot-on-digital-ocean/"/>
    <id>http://blog.url.com/posts/2017/01/21/how-i-deployed-golang-bot-on-digital-ocean/</id>
    <published>2017-01-20T11:00:00-05:00</published>
    <updated>2017-09-30T20:53:19-04:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Continuing where I left off in the &lt;a href="https://aranair.github.io/posts/2016/12/25/how-to-set-up-golang-telegram-bot-with-webhooks/"&gt;first part&lt;/a&gt; of my &lt;code&gt;Golang Telegram Bot&lt;/code&gt;, in this post I go through all the
steps I took to get to a one command deployment of my Telegram bot into a Digital Ocean Ubuntu 16.04
instance. A number of components were involved: dockerizing the app, setting up a self-signed SSL cert,
get the Nginx to work as a reverse proxy with that, submitting the SSL to Telegram, and finally setting up
git post-update webhooks for deployment.&lt;/p&gt;

&lt;h3&gt;Dockerizing the App&lt;/h3&gt;

&lt;p&gt;To prepare the bot for deployment, a natural choice was to dockerize it. I&amp;rsquo;ve found it simpler, by far, to use docker 
to get the environment needed for my apps to run instead of manually fiddling with the server in most cases. It
also gives me the benefit of being able to run multiple containers on a single instance if the load
isn&amp;rsquo;t too high on them. Let&amp;rsquo;s dive in to the code.&lt;/p&gt;

&lt;h5&gt;Dockerfile&lt;/h5&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="s"&gt;FROM golang:1.6&lt;/span&gt;

&lt;span class="s"&gt;ADD configs.toml /go/bin/&lt;/span&gt;

&lt;span class="s"&gt;ADD . /go/src/github.com/aranair/remindbot&lt;/span&gt;
&lt;span class="s"&gt;WORKDIR /go/src/github.com/aranair/remindbot&lt;/span&gt;

&lt;span class="s"&gt;RUN go get ./...&lt;/span&gt;
&lt;span class="s"&gt;RUN go install ./...&lt;/span&gt;

&lt;span class="s"&gt;WORKDIR /go/bin/&lt;/span&gt;
&lt;span class="s"&gt;ENTRYPOINT remindbot&lt;/span&gt;

&lt;span class="s"&gt;EXPOSE 8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I start from the base &lt;a href="https://github.com/docker-library/golang/blob/master/1.6/Dockerfile"&gt;Golang 1.6 image&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;From there, the next line adds a &lt;code&gt;configs.toml&lt;/code&gt; into the bin folder.
This file contains credentials and configs that my app needs to run. This should be added into the server
manually, so that it doesn&amp;rsquo;t get checked into the github repository for security reasons. &lt;/p&gt;

&lt;p&gt;I took a look at the &lt;a href="https://github.com/docker-library/golang/blob/master/1.6/Dockerfile"&gt;official Golang Dockerfile&lt;/a&gt; and I saw that the default &lt;code&gt;gopath&lt;/code&gt; is &lt;code&gt;/go&lt;/code&gt;.
By adding my config item into &lt;code&gt;/go/bin&lt;/code&gt; folder, I can easily give the app direct access to the file,
without having to provide it an additional arbiturary path to get that file.&lt;/p&gt;

&lt;p&gt;The next line adds the files into the image during the build process.
Previously, I would get the files in a different way. I would use this:&lt;/p&gt;
&lt;pre class="highlight docker"&gt;&lt;code&gt;go get /go/src/github.com/aranair/remindbot
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But it&amp;rsquo;s actually a little easier to do it in the above way:&lt;/p&gt;
&lt;pre class="highlight docker"&gt;&lt;code&gt;&lt;span class="k"&gt;ADD&lt;/span&gt;&lt;span class="s"&gt; . /go/src/github.com/aranair/remindbot&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This would take all the files at &lt;code&gt;.&lt;/code&gt;, the location where &lt;code&gt;docker build&lt;/code&gt; 
would run from, and copy them  into &lt;code&gt;/go/src/github.com/aranair/remindbot&lt;/code&gt; during the build process, 
essentially achieving the same result as &lt;code&gt;go get ...&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s different here is that I don&amp;rsquo;t -have- to remember to push to github before the deployment. 
I also wouldn&amp;rsquo;t need to manually &lt;code&gt;git pull&lt;/code&gt;. The entire deployment process can be contained 
inside the post-update webhook. I&amp;rsquo;ll discuss that in more detail further down.&lt;/p&gt;

&lt;h3&gt;Docker Compose&lt;/h3&gt;

&lt;p&gt;Personally, I hate fiddling with manual running of the containers so I just use docker-compose, 
especially if there is more than one component to worry about. For this bot, there is really only
the mounted volume so that my sqlite3 files won&amp;rsquo;t get flushed on a deployment but I could 
just as easily set-up a PostgreSQL database up with a few simple additions to this file.&lt;/p&gt;
&lt;pre class="highlight yaml"&gt;&lt;code&gt;&lt;span class="na"&gt;version&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="s"&gt;2'&lt;/span&gt;
&lt;span class="na"&gt;services&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
  &lt;span class="na"&gt;gobot&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
    &lt;span class="na"&gt;build&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt; &lt;span class="s"&gt;.&lt;/span&gt;
    &lt;span class="na"&gt;ports&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="s"&gt;8080:8080"&lt;/span&gt;
    &lt;span class="na"&gt;volumes&lt;/span&gt;&lt;span class="pi"&gt;:&lt;/span&gt;
      &lt;span class="pi"&gt;-&lt;/span&gt; &lt;span class="s"&gt;/var/data:/var/data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The section under ports exposes and links the container&amp;rsquo;s port 8080 to the server&amp;rsquo;s port 8080 (&lt;code&gt;HOST::CONTAINER&lt;/code&gt; format). 
More about that can be found in the &lt;a href="https://docs.docker.com/compose/compose-file/#/ports"&gt;compose-file documentations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the volumes, the code above simply tells the container to link the host&amp;rsquo;s &lt;code&gt;/var/data/&lt;/code&gt; to the container&amp;rsquo;s &lt;code&gt;/var/data/&lt;/code&gt;
essentially creating a mounted volume. Again, the format is &lt;code&gt;HOST::CONTAINER&lt;/code&gt;. I store the files there for my 
&lt;code&gt;Sqlite&lt;/code&gt; database and this mounted volume preserves the data on deployments.&lt;/p&gt;

&lt;h3&gt;How to Set-up Self-Signed SSL Certificate&lt;/h3&gt;

&lt;p&gt;One of the main hassles and requirements of the Telegram API is that they require https, and that requires
an SSL certificate. It encrypts communication between Telegram and my server, and this helps Telegram to
verify that my server is the correct one, and not a bogus one when a potential man-in-the-middle hijacks the 
traffic.&lt;/p&gt;

&lt;p&gt;I could go get an SSL cert from a provider, but in this case, what we&amp;rsquo;re really concerned is the
identity of my server to Telegram and not for users so a self-signed certificate would work just as well.&lt;/p&gt;

&lt;p&gt;I SSH&amp;rsquo;d into the server to create the certs.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/self-signed.key -out /etc/ssl/certs/self-signed.crt
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-days&lt;/code&gt; defines the validity span. I did adjust the validity-days to something much bigger but generally might be better with a year or two.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rsa:2048&lt;/code&gt; means that will create an RSA key that is 2048 bits long.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-keyout&lt;/code&gt; refers to the private key for the cert&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-out&lt;/code&gt; refers the cert itself&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The process leads me through the steps above for some further information. The most important bit
is the &lt;strong&gt;Common Name&lt;/strong&gt;. In my case, I didn&amp;rsquo;t have a domain, so I simply put in the &lt;code&gt;server_IP_address&lt;/code&gt; 
for my Digital Ocean instance.&lt;/p&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;Country Name (2 letter code) [AU]:SG
State or Province Name (full name) [Some-State]:Singapore
Locality Name (eg, city) []:Singapore
Organization Name (eg, company) [Internet Widgits Pty Ltd]:
Organizational Unit Name (eg, section) []:
Common Name (e.g. server FQDN or YOUR name) []:server_IP_address
Email Address []:boa.homan@gmail.com
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As part of Nginx best practices, I also created a strong Diffie-Hellman group for added security.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;sudo openssl dhparam -out /etc/nginx/ssl/dhparam.pem 2048
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Configuring Nginx to Use the SSL Cert&lt;/h3&gt;

&lt;p&gt;I first made a new Nginx configuration snippet at &lt;code&gt;/etc/nginx/snippets&lt;/code&gt; that simply points
to the SSL certs I just created above.&lt;/p&gt;
&lt;pre class="highlight conf"&gt;&lt;code&gt;&lt;span class="n"&gt;ssl_certificate&lt;/span&gt; /&lt;span class="n"&gt;etc&lt;/span&gt;/&lt;span class="n"&gt;ssl&lt;/span&gt;/&lt;span class="n"&gt;certs&lt;/span&gt;/&lt;span class="n"&gt;self&lt;/span&gt;-&lt;span class="n"&gt;signed&lt;/span&gt;.&lt;span class="n"&gt;crt&lt;/span&gt;;
&lt;span class="n"&gt;ssl_certificate_key&lt;/span&gt; /&lt;span class="n"&gt;etc&lt;/span&gt;/&lt;span class="n"&gt;ssl&lt;/span&gt;/&lt;span class="n"&gt;private&lt;/span&gt;/&lt;span class="n"&gt;self&lt;/span&gt;-&lt;span class="n"&gt;signed&lt;/span&gt;.&lt;span class="n"&gt;key&lt;/span&gt;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another snippet to set-up some SSL settings based on recommendations from &lt;a href="https://cipherli.st/"&gt;https://cipherli.st/&lt;/a&gt;.&lt;/p&gt;
&lt;pre class="highlight conf"&gt;&lt;code&gt;&lt;span class="c"&gt;# from https://cipherli.st/
# and https://raymii.org/s/tutorials/Strong_SSL_Security_On_nginx.html
&lt;/span&gt;
&lt;span class="n"&gt;ssl_protocols&lt;/span&gt; &lt;span class="n"&gt;TLSv1&lt;/span&gt; &lt;span class="n"&gt;TLSv1&lt;/span&gt;.&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="n"&gt;TLSv1&lt;/span&gt;.&lt;span class="m"&gt;2&lt;/span&gt;;
&lt;span class="n"&gt;ssl_prefer_server_ciphers&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;;
&lt;span class="n"&gt;ssl_ciphers&lt;/span&gt; &lt;span class="s2"&gt;"EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH"&lt;/span&gt;;
&lt;span class="n"&gt;ssl_ecdh_curve&lt;/span&gt; &lt;span class="n"&gt;secp384r1&lt;/span&gt;;
&lt;span class="n"&gt;ssl_session_cache&lt;/span&gt; &lt;span class="n"&gt;shared&lt;/span&gt;:&lt;span class="n"&gt;SSL&lt;/span&gt;:&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;;
&lt;span class="n"&gt;ssl_session_tickets&lt;/span&gt; &lt;span class="n"&gt;off&lt;/span&gt;;
&lt;span class="n"&gt;ssl_stapling&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;;
&lt;span class="n"&gt;ssl_stapling_verify&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;;
&lt;span class="n"&gt;resolver&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;.&lt;span class="m"&gt;8&lt;/span&gt;.&lt;span class="m"&gt;8&lt;/span&gt;.&lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;.&lt;span class="m"&gt;8&lt;/span&gt;.&lt;span class="m"&gt;4&lt;/span&gt;.&lt;span class="m"&gt;4&lt;/span&gt; &lt;span class="n"&gt;valid&lt;/span&gt;=&lt;span class="m"&gt;300&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;;
&lt;span class="n"&gt;resolver_timeout&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;;
&lt;span class="c"&gt;# Disable preloading HSTS for now.  You can use the commented out header line that includes
# the "preload" directive if you understand the implications.
#add_header Strict-Transport-Security "max-age=63072000; includeSubdomains; preload";
&lt;/span&gt;&lt;span class="n"&gt;add_header&lt;/span&gt; &lt;span class="n"&gt;Strict&lt;/span&gt;-&lt;span class="n"&gt;Transport&lt;/span&gt;-&lt;span class="n"&gt;Security&lt;/span&gt; &lt;span class="s2"&gt;"max-age=63072000; includeSubdomains"&lt;/span&gt;;
&lt;span class="n"&gt;add_header&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;-&lt;span class="n"&gt;Frame&lt;/span&gt;-&lt;span class="n"&gt;Options&lt;/span&gt; &lt;span class="n"&gt;DENY&lt;/span&gt;;
&lt;span class="n"&gt;add_header&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;-&lt;span class="n"&gt;Content&lt;/span&gt;-&lt;span class="n"&gt;Type&lt;/span&gt;-&lt;span class="n"&gt;Options&lt;/span&gt; &lt;span class="n"&gt;nosniff&lt;/span&gt;;

&lt;span class="n"&gt;ssl_dhparam&lt;/span&gt; /&lt;span class="n"&gt;etc&lt;/span&gt;/&lt;span class="n"&gt;ssl&lt;/span&gt;/&lt;span class="n"&gt;certs&lt;/span&gt;/&lt;span class="n"&gt;dhparam&lt;/span&gt;.&lt;span class="n"&gt;pem&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we will need to edit the Nginx configuration files to use SSL.&lt;/p&gt;

&lt;p&gt;This is what the file intially looks like: &lt;/p&gt;
&lt;pre class="highlight conf"&gt;&lt;code&gt;&lt;span class="c"&gt;# /etc/nginx/sites-available/default
&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt; {
    &lt;span class="n"&gt;listen&lt;/span&gt; &lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;
    &lt;span class="n"&gt;listen&lt;/span&gt; [::]:&lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;

    &lt;span class="c"&gt;# SSL configuration
&lt;/span&gt;
    &lt;span class="c"&gt;# listen 443 ssl default_server;
&lt;/span&gt;    &lt;span class="c"&gt;# listen [::]:443 ssl default_server;
&lt;/span&gt;
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I changed it to look like this (remember to replace IP!)&lt;/p&gt;
&lt;pre class="highlight conf"&gt;&lt;code&gt;&lt;span class="n"&gt;server&lt;/span&gt; {
  &lt;span class="n"&gt;listen&lt;/span&gt; &lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;
  &lt;span class="n"&gt;listen&lt;/span&gt; [::]:&lt;span class="m"&gt;80&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;
  &lt;span class="n"&gt;server_name&lt;/span&gt; &lt;span class="n"&gt;YOUR_SERVER_IP_ADDRESS&lt;/span&gt;;
  &lt;span class="n"&gt;return&lt;/span&gt; &lt;span class="m"&gt;301&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;://$&lt;span class="n"&gt;server_name&lt;/span&gt;$&lt;span class="n"&gt;request_uri&lt;/span&gt;;
}

&lt;span class="n"&gt;server&lt;/span&gt; {
&lt;span class="c"&gt;# SSL configuration
&lt;/span&gt;
  &lt;span class="n"&gt;listen&lt;/span&gt; &lt;span class="m"&gt;443&lt;/span&gt; &lt;span class="n"&gt;ssl&lt;/span&gt; &lt;span class="n"&gt;http2&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;
  &lt;span class="n"&gt;listen&lt;/span&gt; [::]:&lt;span class="m"&gt;443&lt;/span&gt; &lt;span class="n"&gt;ssl&lt;/span&gt; &lt;span class="n"&gt;http2&lt;/span&gt; &lt;span class="n"&gt;default_server&lt;/span&gt;;

  &lt;span class="n"&gt;include&lt;/span&gt; &lt;span class="n"&gt;snippets&lt;/span&gt;/&lt;span class="n"&gt;self&lt;/span&gt;-&lt;span class="n"&gt;signed&lt;/span&gt;.&lt;span class="n"&gt;conf&lt;/span&gt;;
  &lt;span class="n"&gt;include&lt;/span&gt; &lt;span class="n"&gt;snippets&lt;/span&gt;/&lt;span class="n"&gt;ssl&lt;/span&gt;-&lt;span class="n"&gt;params&lt;/span&gt;.&lt;span class="n"&gt;conf&lt;/span&gt;;

  &lt;span class="n"&gt;location&lt;/span&gt; / {
    &lt;span class="n"&gt;proxy_pass&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;://&lt;span class="m"&gt;127&lt;/span&gt;.&lt;span class="m"&gt;0&lt;/span&gt;.&lt;span class="m"&gt;0&lt;/span&gt;.&lt;span class="m"&gt;1&lt;/span&gt;:&lt;span class="m"&gt;8080&lt;/span&gt;;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What I did was to basically ask Nginx to automatically redirect HTTP requests to HTTPS. And ask it to
server root from the port 8080 that the docker container is listening to.&lt;/p&gt;

&lt;h3&gt;UFW Firewall&lt;/h3&gt;

&lt;p&gt;For security reasons, I also enabled &lt;a href="https://help.ubuntu.com/community/UFW"&gt;ufw firewall&lt;/a&gt; by doing the following: &lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;sudo ufw &lt;span class="nb"&gt;enable
&lt;/span&gt;sudo ufw allow &lt;span class="s1"&gt;'OpenSSH'&lt;/span&gt;
sudo ufw allow &lt;span class="s1"&gt;'Nginx Full'&lt;/span&gt;
sudo ufw delete allow &lt;span class="s1"&gt;'Nginx HTTP'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should look something like this in &lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;To                         Action      From
--                         ------      ----
Nginx Full                 ALLOW       Anywhere
OpenSSH                    ALLOW       Anywhere
Nginx Full &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;            ALLOW       Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;
OpenSSH &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;               ALLOW       Anywhere &lt;span class="o"&gt;(&lt;/span&gt;v6&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And its done; to start Nginx, all that&amp;rsquo;s left is to &lt;code&gt;sudo nginx -t&lt;/code&gt;&lt;/p&gt;

&lt;h3&gt;Sending Telegram the SSL Cert&lt;/h3&gt;

&lt;p&gt;Telegram will need the other side of the cert. Consulting their documentation, it seems that they
need the PEM file. To get that, I had to convert the current CRT file into the PEM format.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;openssl x509 -in /etc/ssl/certs/self-signed.crt -outform pem -out /etc/ssl/certs/bot.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the cert to Telegram&amp;rsquo;s hands, I sent it via their API using this: (Replace bot keys!)&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;curl -F &lt;span class="s2"&gt;"url=https://your.domain.or.ip.com"&lt;/span&gt; -F &lt;span class="s2"&gt;"certificate=@/etc/ssl/certs/bot.pem"&lt;/span&gt; https://api.telegram.org/bot12345:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/setWebhook
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Set-up Git Hooks&lt;/h3&gt;

&lt;p&gt;Great! Now all that&amp;rsquo;s left is the deployment process. The general outcome that I wanted is that
it&amp;rsquo;ll be a one-command process that updates the code in the server, rebuilds and restarts the docker
container with the updated app.&lt;/p&gt;

&lt;p&gt;At my server, I created the necessary files for the server git repository&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; /var
mkdir repo &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd &lt;/span&gt;repo
mkdir bot.git &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd &lt;/span&gt;bot.git
git init --bare
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;The Hooks&lt;/h4&gt;

&lt;p&gt;Looking into the &lt;code&gt;hooks&lt;/code&gt; folder in bot.git, there were many samples for the different hooks provided.
For the purpose of this bot, I created a &lt;code&gt;post-receive&lt;/code&gt; hook with the following content.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;&lt;span class="c"&gt;#!/bin/sh&lt;/span&gt;
git --work-tree&lt;span class="o"&gt;=&lt;/span&gt;/var/app/remindbot --git-dir&lt;span class="o"&gt;=&lt;/span&gt;/var/repo/bot.git checkout -f
&lt;span class="nb"&gt;cd&lt;/span&gt; /var/app/remindbot
docker-compose build
docker-compose down
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sets the &lt;code&gt;/var/app&lt;/code&gt; folder to be the working directory for my app. And the script goes into
my working directory and rebuilds the container and restarts it. All of this will happen on deployment!&lt;/p&gt;

&lt;p&gt;Of course, I also had to make the post-receive file executable.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;chmod +x post-receive
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Deploy All The Things!&lt;/h3&gt;

&lt;p&gt;From my development machine, I added a remote repo to my local git repo.&lt;/p&gt;
&lt;pre class="highlight shell"&gt;&lt;code&gt;git remote add prod ssh://user@my.domain.or.ip.com/var/repo/bot.git
git push prod master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the Telegram bot finally goes live, responding to live chats in Telegram. All of this code can be found 
at the &lt;a href="https://github.com/aranair/remindbot"&gt;project github repository&lt;/a&gt;. The reminder bot&amp;rsquo;s name is Hazel. She responds instantly to 
chats in Telegram and helps me to manage my ever-growing list of responsibilities everyday now :P&lt;/p&gt;

&lt;p&gt;Feel free to star it or fork it or leave comments below!&lt;/p&gt;
</content>
  </entry>
</feed>
